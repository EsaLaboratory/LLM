# LLM interface between user, data and optimization model for demand side flexibility

## Table of contents
* Description
* Project structure
* Instalation
* Documentation
* Usage

## Description

This project defines an interface between a user (humand or LLM based), an Large Language Model (LLM) agent and a home energy management system (HEMS). The goal is to provide make home energy management accessible for everyone.

## Project structure

Here is a description of the project architecture

* data
    * default folder where data is saved
* doc
    * folder containing all documentation
* img
    * default folder where the figures are saved
* scripts
    * graph.py
        * Extract test data and create graphs
    * home_management.py
        * Defines the optimization problem
    * main.py
        * Call the global pipeline
    * optim.py
        * Create and model a default user
    * precision.py
        * LLM user precision testing script
    * react.py
        * Defines the LLM agent
* llm_test.sh
    * slurm file to test repo
* ReAct.ipynb
    * interface jupyter notebook 
* README.md
* requirements.txt
    * list all required libraries in pip format

## Instalation

Several installation are possible. We will describe a simple one. Please consider that they are many dependencies, you will have to have sufficient storage to install and use the code of this repository. Python 3.11 is required.

with pip :
``` bash
python3 -m venv env
source env/bin/activate
pip install -r requirements.txt
```

## Documentation

The documentation is generated by the sphinx library. You should activate a python environment with sphinx installed. Here is a list of the command to access the html version :

``` bash
cd doc
make html
python -m http.server
```

Please note that in windows powershell you should use ```.\make``` instead of ```make```.

The documentation will then be available on the following [link](http://localhost:8000/build/html/index.html).

The html file is also accessible here : ./doc/build/html/index.html.

## Usage

This repositery defines multiples functions and class.

To try the global pipeline without going into the full definition of the different tools, a basic usage of the scripts would be to run the main.py file. However take into account that using LLM requires a lot of RAM and memory (the LLM used in the scripts needs 15G). You will also need a GPU to make the LLM's inference faster.

The first thing you can try is to test this [google colab notebook](https://colab.research.google.com/drive/1L43Q2GwPmmTT01gUzhhLRDjt2RTUKqOD?usp=sharing) or the jupyter notebook ReAct.ipynb of the repository.

On line 43 of scripts main.py, you should add your hugging face access token:
``` python
access_token = ""  # add your own hugging face acces token
```

With your environment activated and with sufficient space you can execute this code to test the interface.
``` bash
python main.py --model 'mistralai/Mistral-7B-Instruct-v0.2' --n 20 --difficulty "easy"
```
Arguments description:
* ```model``` refers to the LLM you will use for the LLM agent and user
* ```n``` is the number of tests
* ```difficulty``` LLM user difficulty level of the test(s) : easy, medium, hard or real (you will have to interact with the LLM agent with this mode)
* ```task0``` refers to the tasks index of the tests in case you want to test only some parameters (between 0 and 7)
* ```task1``` refers to the tasks index of the tests (between 0 and 7). task1 must be bigger than task0.
* ```react``` is the system prompt specification: 'react', 'noreact_example', 'noreact'

```main.py``` will call two functions:  ```test_user_retriever``` and ```management_system```. The first one is testing the parameter retrieval part with user/agent interactions. The second one is the main interface, parameter retrieval and HEMS optimization.

For people having access to Oxford's supercomputer, is recommanded to use the repository with arc htc (change USERNAME with your arc username).
``` bash
ssh -X USERNAME@htc-login.arc.ox.ac.uk
```

After you have cloned the repo, you can try the scripts by submitting a slurm file to the cluster.
``` bash
sbatch llm_test.sh
```

This will creates a user with random settings (modeled by an LLM), an LLM agent that will ask questions to the user and retrieve information (ex: weather forecast) to find the problem's parameters and then the problem will be optimized.

You will be able to monitor the scripts with this command:
``` bash
squeue -u $username
```
Or you can also check the `slurm-$jobid.out` file. When the job is done, you will see the modelling output in the `img` folder and in the `data` folder all the retrieved information will be stored in the file `args.json`.